{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ping_Pong.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "hide_code_all_hidden": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoozbehSanaei/reinforcement-learning/blob/main/Ping_Pong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ghlHAH0Nlhi"
      },
      "source": [
        "---\n",
        "DEEP REINFORCEMENT LEARNING EXPLAINED - 20\n",
        "# **RL frameworks**\n",
        "### RLlib: Scalable Reinforcement Learning using Ray\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F1MURpvDQ7Z"
      },
      "source": [
        "Installing ray package and uninstall pyarrow: We must **restart the runtime**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHNPcnXPNlhj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd91872-22e6-4fee-87ff-2c2c5953d41e"
      },
      "source": [
        "#Installing all the dependences\n",
        "!pip uninstall -y pyarrow  > /dev/null\n",
        "#!pip install ray[debug]==0.7.5  > /dev/null 2>&1\n",
        "! pip install -U ray[rllib] &> /dev/null\n",
        "!pip install bs4  > /dev/null 2>&1\n",
        "\n",
        "import os\n",
        "os._exit(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping pyarrow as it is not installed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfc70JI95JK7"
      },
      "source": [
        "\n",
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9Kwo5ZfNlhn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee686863-b638-4604-cb8b-1822530befbf"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "import gym\n",
        "import ray\n",
        "\n",
        "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
        "import os\n",
        "\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "  \"update your install command.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/internal/variadic_reduce.py:115: calling function (from tensorflow.python.eager.def_function) with experimental_compile is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "experimental_compile is deprecated, use jit_compile instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2VTWuLpItqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352deb0b-713a-4bfa-c96b-ac08482ad02d"
      },
      "source": [
        "# Configuration image rendering in colab\n",
        "\n",
        "# Taken from https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7\n",
        "\n",
        "!apt-get install -y xvfb x11-utils &> /dev/null\n",
        "!pip install pyvirtualdisplay==0.2.*  PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.* &> /dev/null\n",
        "!pip install gym[box2d]==0.17.* &> /dev/null\n",
        "\n",
        "# Taken from https://github.com/actions/virtual-environments/issues/214\n",
        "!sudo apt-get update  &> /dev/null \n",
        "!sudo apt-get install xvfb --fix-missing &> /dev/null\n",
        "\n",
        "import pyvirtualdisplay\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "!echo $DISPLAY # sanity checking: should be set to some value (e.g. 1005)\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ":1001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8XsB_qEDnMo"
      },
      "source": [
        "Start up Ray"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQFzEX2BNlh3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "793ba852-9fe9-4383-ca81-2a8b1dfb98a0"
      },
      "source": [
        "ray.init()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-14 20:02:27,832\tINFO services.py:1274 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'metrics_export_port': 63232,\n",
              " 'node_id': '7c55af0055974e2d551c315e59af02bc3eecf50aef2a565e4b7d2f66',\n",
              " 'node_ip_address': '172.28.0.2',\n",
              " 'object_store_address': '/tmp/ray/session_2021-06-14_20-02-26_150225_237/sockets/plasma_store',\n",
              " 'raylet_ip_address': '172.28.0.2',\n",
              " 'raylet_socket_name': '/tmp/ray/session_2021-06-14_20-02-26_150225_237/sockets/raylet',\n",
              " 'redis_address': '172.28.0.2:6379',\n",
              " 'session_dir': '/tmp/ray/session_2021-06-14_20-02-26_150225_237',\n",
              " 'webui_url': '127.0.0.1:8265'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9yhpJZVNlh5"
      },
      "source": [
        "\n",
        "Instantiate a `PPOTrainer` object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok210MCfNlh5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f88e8f56-6a61-4961-959d-b1edb928c6c8"
      },
      "source": [
        "config = DEFAULT_CONFIG.copy()\n",
        "config[\"num_gpus\"] = 1 # in order to use the GPU\n",
        "\n",
        "agent = PPOTrainer(config, 'CartPole-v0')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-14 20:02:33,631\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
            "2021-06-14 20:02:33,632\tINFO trainer.py:698 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "\u001b[2m\u001b[36m(pid=1647)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/internal/variadic_reduce.py:115: calling function (from tensorflow.python.eager.def_function) with experimental_compile is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=1647)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=1647)\u001b[0m experimental_compile is deprecated, use jit_compile instead\n",
            "\u001b[2m\u001b[36m(pid=1646)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/internal/variadic_reduce.py:115: calling function (from tensorflow.python.eager.def_function) with experimental_compile is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=1646)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=1646)\u001b[0m experimental_compile is deprecated, use jit_compile instead\n",
            "2021-06-14 20:02:52,824\tINFO trainable.py:104 -- Trainable.setup took 19.195 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2021-06-14 20:02:52,826\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qHoH5knw6Uk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06718c6a-6406-498a-cb8b-9f9043b247d4"
      },
      "source": [
        "print(DEFAULT_CONFIG)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'num_workers': 2, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'num_framestacks': 'auto', 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': None, 'env_config': {}, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 5e-05, 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, 'simple_optimizer': -1, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeyYiHJY8yIj"
      },
      "source": [
        "Watch Agent before training using `compute_action()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wH9rJkP8xB3"
      },
      "source": [
        "H = 200  # The number of hidden layer neurons.\n",
        "gamma = 0.99  # The discount factor for reward.\n",
        "decay_rate = 0.99  # The decay factor for RMSProp leaky sum of grad^2.\n",
        "D = 80 * 80  # The input dimensionality: 80x80 grid.\n",
        "learning_rate = 1e-4  # Magnitude of the update."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN9nVl-7D3Ys"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mbhigVYpYiA"
      },
      "source": [
        "def preprocess(img):\n",
        "    # Crop the image.\n",
        "    img = img[35:195]\n",
        "    # Downsample by factor of 2.\n",
        "    img = img[::2, ::2, 0]\n",
        "    # Erase background (background type 1).\n",
        "    img[img == 144] = 0\n",
        "    # Erase background (background type 2).\n",
        "    img[img == 109] = 0\n",
        "    # Set everything else (paddles, ball) to 1.\n",
        "    img[img != 0] = 1\n",
        "    return img.astype(np.float).ravel()\n",
        "\n",
        "\n",
        "def process_rewards(r):\n",
        "    \"\"\"Compute discounted reward from a vector of rewards.\"\"\"\n",
        "    discounted_r = np.zeros_like(r)\n",
        "    running_add = 0\n",
        "    for t in reversed(range(0, r.size)):\n",
        "        # Reset the sum, since this was a game boundary (pong specific!).\n",
        "        if r[t] != 0:\n",
        "            running_add = 0\n",
        "        running_add = running_add * gamma + r[t]\n",
        "        discounted_r[t] = running_add\n",
        "    return discounted_r\n",
        "\n",
        "\n",
        "def rollout(model, env):\n",
        "    \"\"\"Evaluates  env and model until the env returns \"Done\".\n",
        "\n",
        "    Returns:\n",
        "        xs: A list of observations\n",
        "        hs: A list of model hidden states per observation\n",
        "        dlogps: A list of gradients\n",
        "        drs: A list of rewards.\n",
        "\n",
        "    \"\"\"\n",
        "    # Reset the game.\n",
        "    observation = env.reset()\n",
        "    # Note that prev_x is used in computing the difference frame.\n",
        "    prev_x = None\n",
        "    xs, hs, dlogps, drs = [], [], [], []\n",
        "    done = False\n",
        "    while not done:\n",
        "        cur_x = preprocess(observation)\n",
        "        x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "        prev_x = cur_x\n",
        "\n",
        "        aprob, h = model.policy_forward(x)\n",
        "        # Sample an action.\n",
        "        action = 2 if np.random.uniform() < aprob else 3\n",
        "\n",
        "        # The observation.\n",
        "        xs.append(x)\n",
        "        # The hidden state.\n",
        "        hs.append(h)\n",
        "        y = 1 if action == 2 else 0  # A \"fake label\".\n",
        "        # The gradient that encourages the action that was taken to be\n",
        "        # taken (see http://cs231n.github.io/neural-networks-2/#losses if\n",
        "        # confused).\n",
        "        dlogps.append(y - aprob)\n",
        "\n",
        "        observation, reward, done, info = env.step(action)\n",
        "\n",
        "        # Record reward (has to be done after we call step() to get reward\n",
        "        # for previous action).\n",
        "        drs.append(reward)\n",
        "    return xs, hs, dlogps, drs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HenGbKl4Lo4x"
      },
      "source": [
        "class Model(object):\n",
        "    \"\"\"This class holds the neural network weights.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.weights = {}\n",
        "        self.weights[\"W1\"] = np.random.randn(H, D) / np.sqrt(D)\n",
        "        self.weights[\"W2\"] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "    def policy_forward(self, x):\n",
        "        h = np.dot(self.weights[\"W1\"], x)\n",
        "        h[h < 0] = 0  # ReLU nonlinearity.\n",
        "        logp = np.dot(self.weights[\"W2\"], h)\n",
        "        # Softmax\n",
        "        p = 1.0 / (1.0 + np.exp(-logp))\n",
        "        # Return probability of taking action 2, and hidden state.\n",
        "        return p, h\n",
        "\n",
        "    def policy_backward(self, eph, epx, epdlogp):\n",
        "        \"\"\"Backward pass to calculate gradients.\n",
        "\n",
        "        Arguments:\n",
        "            eph: Array of intermediate hidden states.\n",
        "            epx: Array of experiences (observations.\n",
        "            epdlogp: Array of logps (output of last layer before softmax/\n",
        "\n",
        "        \"\"\"\n",
        "        dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "        dh = np.outer(epdlogp, self.weights[\"W2\"])\n",
        "        # Backprop relu.\n",
        "        dh[eph <= 0] = 0\n",
        "        dW1 = np.dot(dh.T, epx)\n",
        "        return {\"W1\": dW1, \"W2\": dW2}\n",
        "\n",
        "    def update(self, grad_buffer, rmsprop_cache, lr, decay):\n",
        "        \"\"\"Applies the gradients to the model parameters with RMSProp.\"\"\"\n",
        "        for k, v in self.weights.items():\n",
        "            g = grad_buffer[k]\n",
        "            rmsprop_cache[k] = (decay * rmsprop_cache[k] + (1 - decay) * g**2)\n",
        "            self.weights[k] += lr * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "\n",
        "\n",
        "def zero_grads(grad_buffer):\n",
        "    \"\"\"Reset the batch gradient buffer.\"\"\"\n",
        "    for k, v in grad_buffer.items():\n",
        "        grad_buffer[k] = np.zeros_like(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W10jhixIQI7l"
      },
      "source": [
        "#ray.init()\n",
        "\n",
        "\n",
        "@ray.remote\n",
        "class RolloutWorker(object):\n",
        "    def __init__(self):\n",
        "        # Tell numpy to only use one core. If we don't do this, each actor may\n",
        "        # try to use all of the cores and the resulting contention may result\n",
        "        # in no speedup over the serial version. Note that if numpy is using\n",
        "        # OpenBLAS, then you need to set OPENBLAS_NUM_THREADS=1, and you\n",
        "        # probably need to do it from the command line (so it happens before\n",
        "        # numpy is imported).\n",
        "        os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
        "        self.env = gym.make(\"Pong-v0\")\n",
        "\n",
        "    def compute_gradient(self, model):\n",
        "        # Compute a simulation episode.\n",
        "        xs, hs, dlogps, drs = rollout(model, self.env)\n",
        "        reward_sum = sum(drs)\n",
        "        # Vectorize the arrays.\n",
        "        epx = np.vstack(xs)\n",
        "        eph = np.vstack(hs)\n",
        "        epdlogp = np.vstack(dlogps)\n",
        "        epr = np.vstack(drs)\n",
        "\n",
        "        # Compute the discounted reward backward through time.\n",
        "        discounted_epr = process_rewards(epr)\n",
        "        # Standardize the rewards to be unit normal (helps control the gradient\n",
        "        # estimator variance).\n",
        "        discounted_epr -= np.mean(discounted_epr)\n",
        "        discounted_epr /= np.std(discounted_epr)\n",
        "        # Modulate the gradient with advantage (the policy gradient magic\n",
        "        # happens right here).\n",
        "        epdlogp *= discounted_epr\n",
        "        return model.policy_backward(eph, epx, epdlogp), reward_sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_9qxk-mMxxa",
        "outputId": "3ce092ce-8e24-44e4-cd76-5c4966718d83"
      },
      "source": [
        "#!wget http://www.atarimania.com/roms/Roms.rar\n",
        "#!unzip Roms.rar\n",
        "#!apt-get install unrar\n",
        "#!unrar e -r Roms.rar\n",
        "#!unzip ROMS.zip\n",
        "#!ls ROMS/*\n",
        "#!python -m atari_py.import_roms ROMS\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "copying pooyan.bin from ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying wizard_of_wor.bin from ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying pong.bin from ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying kangaroo.bin from ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying seaquest.bin from ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying phoenix.bin from ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying star_gunner.bin from ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying boxing.bin from ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying keystone_kapers.bin from ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying zaxxon.bin from ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n",
            "copying riverraid.bin from ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying venture.bin from ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying yars_revenge.bin from ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying name_this_game.bin from ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying montezuma_revenge.bin from ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying assault.bin from ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying video_pinball.bin from ROMS/Video Pinball - Arcade Pinball (1981) (Atari, Bob Smith - Sears) (CX2648 - 49-75161) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying hero.bin from ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying tennis.bin from ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying krull.bin from ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying king_kong.bin from ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying amidar.bin from ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying demon_attack.bin from ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying bowling.bin from ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying pacman.bin from ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying qbert.bin from ROMS/Q-bert (1987) (Atari) (CX26150).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "copying space_invaders.bin from ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying private_eye.bin from ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying freeway.bin from ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying gravitar.bin from ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying battle_zone.bin from ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying solaris.bin from ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying centipede.bin from ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying gopher.bin from ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying jamesbond.bin from ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying beam_rider.bin from ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying mr_do.bin from ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying ice_hockey.bin from ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying lost_luggage.bin from ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying tutankham.bin from ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying donkey_kong.bin from ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying pitfall.bin from ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying surround.bin from ROMS/Surround - Chase (Blockade) (1977) (Atari, Alan Miller - Sears) (CX2641 - 99807, 49-75105) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying elevator_action.bin from ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying berzerk.bin from ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying laser_gates.bin from ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying alien.bin from ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying kung_fu_master.bin from ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying up_n_down.bin from ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying asteroids.bin from ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying crazy_climber.bin from ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying skiing.bin from ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying kaboom.bin from ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying ms_pacman.bin from ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying galaxian.bin from ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying fishing_derby.bin from ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying breakout.bin from ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying chopper_command.bin from ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying robotank.bin from ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying atlantis.bin from ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying enduro.bin from ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying defender.bin from ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying air_raid.bin from ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying koolaid.bin from ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying trondead.bin from ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying double_dunk.bin from ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying frostbite.bin from ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying frogger.bin from ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying road_runner.bin from patched version of ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying journey_escape.bin from ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying adventure.bin from ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying sir_lancelot.bin from ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
            "copying time_pilot.bin from ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying carnival.bin from ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying bank_heist.bin from ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8QbI-bQQM6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c5963b-5d9b-4773-afdd-cd6eff9de083"
      },
      "source": [
        "iterations = 20\n",
        "batch_size = 4\n",
        "model = Model()\n",
        "actors = [RolloutWorker.remote() for _ in range(batch_size)]\n",
        "\n",
        "running_reward = None\n",
        "# \"Xavier\" initialization.\n",
        "# Update buffers that add up gradients over a batch.\n",
        "grad_buffer = {k: np.zeros_like(v) for k, v in model.weights.items()}\n",
        "# Update the rmsprop memory.\n",
        "rmsprop_cache = {k: np.zeros_like(v) for k, v in model.weights.items()}\n",
        "\n",
        "for i in range(1, 1 + iterations):\n",
        "    model_id = ray.put(model)\n",
        "    gradient_ids = []\n",
        "    # Launch tasks to compute gradients from multiple rollouts in parallel.\n",
        "    start_time = time.time()\n",
        "    gradient_ids = [\n",
        "        actor.compute_gradient.remote(model_id) for actor in actors\n",
        "    ]\n",
        "    for batch in range(batch_size):\n",
        "        [grad_id], gradient_ids = ray.wait(gradient_ids)\n",
        "        grad, reward_sum = ray.get(grad_id)\n",
        "        # Accumulate the gradient over batch.\n",
        "        for k in model.weights:\n",
        "            grad_buffer[k] += grad[k]\n",
        "        running_reward = (reward_sum if running_reward is None else\n",
        "                          running_reward * 0.99 + reward_sum * 0.01)\n",
        "    end_time = time.time()\n",
        "    print(\"Batch {} computed {} rollouts in {} seconds, \"\n",
        "          \"running mean is {}\".format(i, batch_size, end_time - start_time,\n",
        "                                      running_reward))\n",
        "    model.update(grad_buffer, rmsprop_cache, learning_rate, decay_rate)\n",
        "    zero_grads(grad_buffer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "2021-06-14 20:35:35,142\tWARNING worker.py:1114 -- WARNING: 8 PYTHON workers have been started on a node of the id: 7c55af0055974e2d551c315e59af02bc3eecf50aef2a565e4b7d2f66 and address: 172.28.0.2. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 1 computed 4 rollouts in 9.447175025939941 seconds, running mean is -20.009900000000002\n",
            "Batch 2 computed 4 rollouts in 5.863585710525513 seconds, running mean is -20.029112890499\n",
            "Batch 3 computed 4 rollouts in 6.17892050743103 seconds, running mean is -20.047469716452905\n",
            "Batch 4 computed 4 rollouts in 7.361988544464111 seconds, running mean is -20.025601210220497\n",
            "Batch 5 computed 4 rollouts in 7.139967441558838 seconds, running mean is -20.02409641038898\n",
            "Batch 6 computed 4 rollouts in 5.684454679489136 seconds, running mean is -20.052650905674977\n",
            "Batch 7 computed 4 rollouts in 6.29955530166626 seconds, running mean is -20.070179239914268\n",
            "Batch 8 computed 4 rollouts in 6.260870456695557 seconds, running mean is -20.077116887846472\n",
            "Batch 9 computed 4 rollouts in 6.499212741851807 seconds, running mean is -20.063682164768938\n",
            "Batch 10 computed 4 rollouts in 6.908064126968384 seconds, running mean is -20.080676823385204\n",
            "Batch 11 computed 4 rollouts in 6.439464092254639 seconds, running mean is -20.107001824643305\n",
            "Batch 12 computed 4 rollouts in 5.963776111602783 seconds, running mean is -20.14218951581508\n",
            "Batch 13 computed 4 rollouts in 6.226782321929932 seconds, running mean is -20.165990671555797\n",
            "Batch 14 computed 4 rollouts in 6.861469745635986 seconds, running mean is -20.178953966793717\n",
            "Batch 15 computed 4 rollouts in 6.1604108810424805 seconds, running mean is -20.18160545647571\n",
            "Batch 16 computed 4 rollouts in 5.853283405303955 seconds, running mean is -20.2138534668848\n",
            "Batch 17 computed 4 rollouts in 5.998924493789673 seconds, running mean is -20.225029777014207\n",
            "Batch 18 computed 4 rollouts in 5.888022422790527 seconds, running mean is -20.235666695931034\n",
            "Batch 19 computed 4 rollouts in 6.511515378952026 seconds, running mean is -20.255784477801235\n",
            "Batch 20 computed 4 rollouts in 6.59489631652832 seconds, running mean is -20.2752095387958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXyOl29-30mL"
      },
      "source": [
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}