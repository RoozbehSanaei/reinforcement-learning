{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RoozbehSanaei/reinforcement-learning/blob/main/Ping_Pong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ghlHAH0Nlhi"
   },
   "source": [
    "---\n",
    "DEEP REINFORCEMENT LEARNING EXPLAINED - 20\n",
    "# **RL frameworks**\n",
    "### RLlib: Scalable Reinforcement Learning using Ray\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1F1MURpvDQ7Z"
   },
   "source": [
    "Installing ray package and uninstall pyarrow: We must **restart the runtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHNPcnXPNlhj",
    "outputId": "dbd91872-22e6-4fee-87ff-2c2c5953d41e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping pyarrow as it is not installed.\u001b[0m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "#Installing all the dependences\n",
    "!pip uninstall -y pyarrow  > /dev/null\n",
    "#!pip install ray[debug]==0.7.5  > /dev/null 2>&1\n",
    "! pip install -U ray[rllib] &> /dev/null\n",
    "!pip install bs4  > /dev/null 2>&1\n",
    "\n",
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mfc70JI95JK7"
   },
   "source": [
    "\n",
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9Kwo5ZfNlhn",
    "outputId": "ee686863-b638-4604-cb8b-1822530befbf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  \"update your install command.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "import gym\n",
    "import ray\n",
    "\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "import os\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2VTWuLpItqw",
    "outputId": "352deb0b-713a-4bfa-c96b-ac08482ad02d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":1009\r\n"
     ]
    }
   ],
   "source": [
    "# Configuration image rendering in colab\n",
    "\n",
    "# Taken from https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7\n",
    "\n",
    "!apt-get install -y xvfb x11-utils &> /dev/null\n",
    "!pip install pyvirtualdisplay==0.2.*  PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.* &> /dev/null\n",
    "!pip install gym[box2d]==0.17.* &> /dev/null\n",
    "\n",
    "# Taken from https://github.com/actions/virtual-environments/issues/214\n",
    "!sudo apt-get update  &> /dev/null \n",
    "!sudo apt-get install xvfb --fix-missing &> /dev/null\n",
    "\n",
    "import pyvirtualdisplay\n",
    "\n",
    "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
    "_ = _display.start()\n",
    "\n",
    "!echo $DISPLAY # sanity checking: should be set to some value (e.g. 1005)\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8XsB_qEDnMo"
   },
   "source": [
    "Start up Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQFzEX2BNlh3",
    "outputId": "793ba852-9fe9-4383-ca81-2a8b1dfb98a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-15 17:26:42,406\tINFO services.py:1274 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metrics_export_port': 63337,\n",
       " 'node_id': '8da187c3a5c973df396c63ead2c2fd0f435ace15274e90b10eb3048e',\n",
       " 'node_ip_address': '10.221.248.109',\n",
       " 'object_store_address': '/tmp/ray/session_2021-06-15_17-26-40_838183_4763/sockets/plasma_store',\n",
       " 'raylet_ip_address': '10.221.248.109',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-06-15_17-26-40_838183_4763/sockets/raylet',\n",
       " 'redis_address': '10.221.248.109:6379',\n",
       " 'session_dir': '/tmp/ray/session_2021-06-15_17-26-40_838183_4763',\n",
       " 'webui_url': '127.0.0.1:8265'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9yhpJZVNlh5"
   },
   "source": [
    "\n",
    "Instantiate a `PPOTrainer` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ok210MCfNlh5",
    "outputId": "f88e8f56-6a61-4961-959d-b1edb928c6c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-15 17:26:47,852\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-06-15 17:26:47,853\tINFO trainer.py:698 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-06-15 17:26:53,569\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 1 # in order to use the GPU\n",
    "\n",
    "agent = PPOTrainer(config, 'CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qHoH5knw6Uk",
    "outputId": "06718c6a-6406-498a-cb8b-9f9043b247d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_workers': 2, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'num_framestacks': 'auto', 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': None, 'env_config': {}, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 5e-05, 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, 'simple_optimizer': -1, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n"
     ]
    }
   ],
   "source": [
    "print(DEFAULT_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeyYiHJY8yIj"
   },
   "source": [
    "Watch Agent before training using `compute_action()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4wH9rJkP8xB3"
   },
   "outputs": [],
   "source": [
    "H = 200  # The number of hidden layer neurons.\n",
    "gamma = 0.99  # The discount factor for reward.\n",
    "decay_rate = 0.99  # The decay factor for RMSProp leaky sum of grad^2.\n",
    "D = 80 * 80  # The input dimensionality: 80x80 grid.\n",
    "learning_rate = 1e-4  # Magnitude of the update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HN9nVl-7D3Ys"
   },
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-mbhigVYpYiA"
   },
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    # Crop the image.\n",
    "    img = img[35:195]\n",
    "    # Downsample by factor of 2.\n",
    "    img = img[::2, ::2, 0]\n",
    "    # Erase background (background type 1).\n",
    "    img[img == 144] = 0\n",
    "    # Erase background (background type 2).\n",
    "    img[img == 109] = 0\n",
    "    # Set everything else (paddles, ball) to 1.\n",
    "    img[img != 0] = 1\n",
    "    return img.astype(np.float).ravel()\n",
    "\n",
    "\n",
    "def process_rewards(r):\n",
    "    \"\"\"Compute discounted reward from a vector of rewards.\"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        # Reset the sum, since this was a game boundary (pong specific!).\n",
    "        if r[t] != 0:\n",
    "            running_add = 0\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "def rollout(model, env):\n",
    "    \"\"\"Evaluates  env and model until the env returns \"Done\".\n",
    "\n",
    "    Returns:\n",
    "        xs: A list of observations\n",
    "        hs: A list of model hidden states per observation\n",
    "        dlogps: A list of gradients\n",
    "        drs: A list of rewards.\n",
    "\n",
    "    \"\"\"\n",
    "    # Reset the game.\n",
    "    observation = env.reset()\n",
    "    # Note that prev_x is used in computing the difference frame.\n",
    "    prev_x = None\n",
    "    xs, hs, dlogps, drs = [], [], [], []\n",
    "    done = False\n",
    "    while not done:\n",
    "        cur_x = preprocess(observation)\n",
    "        x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "        prev_x = cur_x\n",
    "\n",
    "        aprob, h = model.policy_forward(x)\n",
    "        # Sample an action.\n",
    "        action = 2 if np.random.uniform() < aprob else 3\n",
    "\n",
    "        # The observation.\n",
    "        xs.append(x)\n",
    "        # The hidden state.\n",
    "        hs.append(h)\n",
    "        y = 1 if action == 2 else 0  # A \"fake label\".\n",
    "        # The gradient that encourages the action that was taken to be\n",
    "        # taken (see http://cs231n.github.io/neural-networks-2/#losses if\n",
    "        # confused).\n",
    "        dlogps.append(y - aprob)\n",
    "\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # Record reward (has to be done after we call step() to get reward\n",
    "        # for previous action).\n",
    "        drs.append(reward)\n",
    "    return xs, hs, dlogps, drs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HenGbKl4Lo4x"
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"This class holds the neural network weights.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.weights = {}\n",
    "        self.weights[\"W1\"] = np.random.randn(H, D) / np.sqrt(D)\n",
    "        self.weights[\"W2\"] = np.random.randn(H) / np.sqrt(H)\n",
    "\n",
    "    def policy_forward(self, x):\n",
    "        h = np.dot(self.weights[\"W1\"], x)\n",
    "        h[h < 0] = 0  # ReLU nonlinearity.\n",
    "        logp = np.dot(self.weights[\"W2\"], h)\n",
    "        # Softmax\n",
    "        p = 1.0 / (1.0 + np.exp(-logp))\n",
    "        # Return probability of taking action 2, and hidden state.\n",
    "        return p, h\n",
    "\n",
    "    def policy_backward(self, eph, epx, epdlogp):\n",
    "        \"\"\"Backward pass to calculate gradients.\n",
    "\n",
    "        Arguments:\n",
    "            eph: Array of intermediate hidden states.\n",
    "            epx: Array of experiences (observations.\n",
    "            epdlogp: Array of logps (output of last layer before softmax/\n",
    "\n",
    "        \"\"\"\n",
    "        dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "        dh = np.outer(epdlogp, self.weights[\"W2\"])\n",
    "        # Backprop relu.\n",
    "        dh[eph <= 0] = 0\n",
    "        dW1 = np.dot(dh.T, epx)\n",
    "        return {\"W1\": dW1, \"W2\": dW2}\n",
    "\n",
    "    def update(self, grad_buffer, rmsprop_cache, lr, decay):\n",
    "        \"\"\"Applies the gradients to the model parameters with RMSProp.\"\"\"\n",
    "        for k, v in self.weights.items():\n",
    "            g = grad_buffer[k]\n",
    "            rmsprop_cache[k] = (decay * rmsprop_cache[k] + (1 - decay) * g**2)\n",
    "            self.weights[k] += lr * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "\n",
    "\n",
    "def zero_grads(grad_buffer):\n",
    "    \"\"\"Reset the batch gradient buffer.\"\"\"\n",
    "    for k, v in grad_buffer.items():\n",
    "        grad_buffer[k] = np.zeros_like(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "W10jhixIQI7l"
   },
   "outputs": [],
   "source": [
    "#ray.init()\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class RolloutWorker(object):\n",
    "    def __init__(self):\n",
    "        # Tell numpy to only use one core. If we don't do this, each actor may\n",
    "        # try to use all of the cores and the resulting contention may result\n",
    "        # in no speedup over the serial version. Note that if numpy is using\n",
    "        # OpenBLAS, then you need to set OPENBLAS_NUM_THREADS=1, and you\n",
    "        # probably need to do it from the command line (so it happens before\n",
    "        # numpy is imported).\n",
    "        os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "        self.env = gym.make(\"Pong-v0\")\n",
    "\n",
    "    def compute_gradient(self, model):\n",
    "        # Compute a simulation episode.\n",
    "        xs, hs, dlogps, drs = rollout(model, self.env)\n",
    "        reward_sum = sum(drs)\n",
    "        # Vectorize the arrays.\n",
    "        epx = np.vstack(xs)\n",
    "        eph = np.vstack(hs)\n",
    "        epdlogp = np.vstack(dlogps)\n",
    "        epr = np.vstack(drs)\n",
    "\n",
    "        # Compute the discounted reward backward through time.\n",
    "        discounted_epr = process_rewards(epr)\n",
    "        # Standardize the rewards to be unit normal (helps control the gradient\n",
    "        # estimator variance).\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "        # Modulate the gradient with advantage (the policy gradient magic\n",
    "        # happens right here).\n",
    "        epdlogp *= discounted_epr\n",
    "        return model.policy_backward(eph, epx, epdlogp), reward_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_9qxk-mMxxa",
    "outputId": "3ce092ce-8e24-44e4-cd76-5c4966718d83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python: No module named atari_py\r\n"
     ]
    }
   ],
   "source": [
    "!python -m atari_py.import_roms ROMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8QbI-bQQM6H",
    "outputId": "20c5963b-5d9b-4773-afdd-cd6eff9de083"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m 2021-06-15 17:27:08,568\tERROR worker.py:409 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=4969, ip=10.221.248.109)\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m ModuleNotFoundError: No module named 'atari_py'\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=4969, ip=10.221.248.109)\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"python/ray/_raylet.pyx\", line 490, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"python/ray/_raylet.pyx\", line 497, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"python/ray/_raylet.pyx\", line 501, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"<ipython-input-9-a107929bfe25>\", line 14, in __init__\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 145, in make\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 90, in make\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 59, in make\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m     cls = load(self.entry_point)\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 18, in load\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m     mod = importlib.import_module(mod_name)\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m     return _bootstrap._gcd_import(name[level:], package, level)\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/__init__.py\", line 1, in <module>\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m     from gym.envs.atari.atari_env import AtariEnv\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\", line 13, in <module>\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m     \"'pip install gym[atari]'.)\".format(e))\n",
      "\u001b[2m\u001b[36m(pid=4969)\u001b[0m gym.error.DependencyNotInstalled: No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m 2021-06-15 17:27:08,577\tERROR worker.py:409 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=4980, ip=10.221.248.109)\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m ModuleNotFoundError: No module named 'atari_py'\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=4980, ip=10.221.248.109)\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"python/ray/_raylet.pyx\", line 490, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"python/ray/_raylet.pyx\", line 497, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"python/ray/_raylet.pyx\", line 501, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"<ipython-input-9-a107929bfe25>\", line 14, in __init__\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 145, in make\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 90, in make\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 59, in make\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m     cls = load(self.entry_point)\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 18, in load\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m     mod = importlib.import_module(mod_name)\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m     return _bootstrap._gcd_import(name[level:], package, level)\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/__init__.py\", line 1, in <module>\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m     from gym.envs.atari.atari_env import AtariEnv\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\", line 13, in <module>\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m     \"'pip install gym[atari]'.)\".format(e))\n",
      "\u001b[2m\u001b[36m(pid=4980)\u001b[0m gym.error.DependencyNotInstalled: No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m 2021-06-15 17:27:08,568\tERROR worker.py:409 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=4977, ip=10.221.248.109)\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m ModuleNotFoundError: No module named 'atari_py'\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=4977, ip=10.221.248.109)\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"python/ray/_raylet.pyx\", line 490, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"python/ray/_raylet.pyx\", line 497, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"python/ray/_raylet.pyx\", line 501, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"<ipython-input-9-a107929bfe25>\", line 14, in __init__\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 145, in make\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 90, in make\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 59, in make\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m     cls = load(self.entry_point)\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 18, in load\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m     mod = importlib.import_module(mod_name)\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m     return _bootstrap._gcd_import(name[level:], package, level)\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/__init__.py\", line 1, in <module>\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m     from gym.envs.atari.atari_env import AtariEnv\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\", line 13, in <module>\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m     \"'pip install gym[atari]'.)\".format(e))\n",
      "\u001b[2m\u001b[36m(pid=4977)\u001b[0m gym.error.DependencyNotInstalled: No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m 2021-06-15 17:27:08,571\tERROR worker.py:409 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=4970, ip=10.221.248.109)\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m ModuleNotFoundError: No module named 'atari_py'\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=4970, ip=10.221.248.109)\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"python/ray/_raylet.pyx\", line 490, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"python/ray/_raylet.pyx\", line 497, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"python/ray/_raylet.pyx\", line 501, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"<ipython-input-9-a107929bfe25>\", line 14, in __init__\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 145, in make\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 90, in make\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 59, in make\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m     cls = load(self.entry_point)\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 18, in load\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m     mod = importlib.import_module(mod_name)\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m     return _bootstrap._gcd_import(name[level:], package, level)\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/__init__.py\", line 1, in <module>\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m     from gym.envs.atari.atari_env import AtariEnv\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\", line 13, in <module>\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m     \"'pip install gym[atari]'.)\".format(e))\n",
      "\u001b[2m\u001b[36m(pid=4970)\u001b[0m gym.error.DependencyNotInstalled: No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)\n"
     ]
    },
    {
     "ename": "RayActorError",
     "evalue": "The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=4977, ip=10.221.248.109)\nModuleNotFoundError: No module named 'atari_py'\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=4977, ip=10.221.248.109)\n  File \"python/ray/_raylet.pyx\", line 490, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 497, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 501, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n  File \"/usr/local/lib/python3.6/dist-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"<ipython-input-9-a107929bfe25>\", line 14, in __init__\n  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 145, in make\n    return registry.make(id, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 90, in make\n    env = spec.make(**kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 59, in make\n    cls = load(self.entry_point)\n  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 18, in load\n    mod = importlib.import_module(mod_name)\n  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/__init__.py\", line 1, in <module>\n    from gym.envs.atari.atari_env import AtariEnv\n  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\", line 13, in <module>\n    \"'pip install gym[atari]'.)\".format(e))\ngym.error.DependencyNotInstalled: No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8a24e1b185ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mgrad_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Accumulate the gradient over batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclient_mode_should_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1494\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_individual_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=4977, ip=10.221.248.109)\nModuleNotFoundError: No module named 'atari_py'\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=4977, ip=10.221.248.109)\n  File \"python/ray/_raylet.pyx\", line 490, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 497, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 501, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n  File \"/usr/local/lib/python3.6/dist-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"<ipython-input-9-a107929bfe25>\", line 14, in __init__\n  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 145, in make\n    return registry.make(id, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 90, in make\n    env = spec.make(**kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 59, in make\n    cls = load(self.entry_point)\n  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\", line 18, in load\n    mod = importlib.import_module(mod_name)\n  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/__init__.py\", line 1, in <module>\n    from gym.envs.atari.atari_env import AtariEnv\n  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\", line 13, in <module>\n    \"'pip install gym[atari]'.)\".format(e))\ngym.error.DependencyNotInstalled: No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)"
     ]
    }
   ],
   "source": [
    "iterations = 20\n",
    "batch_size = 4\n",
    "model = Model()\n",
    "actors = [RolloutWorker.remote() for _ in range(batch_size)]\n",
    "\n",
    "running_reward = None\n",
    "# \"Xavier\" initialization.\n",
    "# Update buffers that add up gradients over a batch.\n",
    "grad_buffer = {k: np.zeros_like(v) for k, v in model.weights.items()}\n",
    "# Update the rmsprop memory.\n",
    "rmsprop_cache = {k: np.zeros_like(v) for k, v in model.weights.items()}\n",
    "\n",
    "for i in range(1, 1 + iterations):\n",
    "    model_id = ray.put(model)\n",
    "    gradient_ids = []\n",
    "    # Launch tasks to compute gradients from multiple rollouts in parallel.\n",
    "    start_time = time.time()\n",
    "    gradient_ids = [\n",
    "        actor.compute_gradient.remote(model_id) for actor in actors\n",
    "    ]\n",
    "    for batch in range(batch_size):\n",
    "        [grad_id], gradient_ids = ray.wait(gradient_ids)\n",
    "        grad, reward_sum = ray.get(grad_id)\n",
    "        # Accumulate the gradient over batch.\n",
    "        for k in model.weights:\n",
    "            grad_buffer[k] += grad[k]\n",
    "        running_reward = (reward_sum if running_reward is None else\n",
    "                          running_reward * 0.99 + reward_sum * 0.01)\n",
    "    end_time = time.time()\n",
    "    print(\"Batch {} computed {} rollouts in {} seconds, \"\n",
    "          \"running mean is {}\".format(i, batch_size, end_time - start_time,\n",
    "                                      running_reward))\n",
    "    model.update(grad_buffer, rmsprop_cache, learning_rate, decay_rate)\n",
    "    zero_grads(grad_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'HC ROMS'   Ping_Pong.ipynb   ROMS\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXyOl29-30mL"
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Ping_Pong.ipynb",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
